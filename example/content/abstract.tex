Deploying million-token Large Language Models (LLMs) is challenging because production workloads are highly heterogeneous, mixing short queries and long documents. This heterogeneity, combined with the quadratic complexity of attention, creates severe convoy effects where long-running requests stall short, interactive ones, degrading system responsiveness. We present \sysname, a serving system that eliminates these convoys by introducing fine-grained, preemptive scheduling to LLM inference.

\sysname makes preemption practical with a co-designed set of mechanisms -- including \textit{Adaptive Chunking} and \textit{Stream Pipeline Parallel}-- that overcome the perceived inefficiencies and scaling challenges of chunking. Additionally, we present a new parallelism strategy \textit{KV-Cache Parallelism} to reduce the decode latency and afford interactivity despite very long context.
These mechanisms are orchestrated by a \textit{Length-Aware Relative Slack (LARS)} scheduler, a deadline- and heterogeneity-aware  scheduling policy that prevents both the convoy effect and the starvation that plagues simpler policies. Under a heterogeneous workload, \sysname improves throughput by 5.7\myx while reducing median and 99th-percentile latency by 30\myx and 174\myx, respectively, compared to state-of-the-art non-preemptive systems.
